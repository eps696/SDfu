{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "BCyeZMHNT2E-",
        "yWpWFeyO8APF"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzA1-mt88AO_"
      },
      "source": [
        "# Stable Diffusion on diffusers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IFfx8GQIAQm"
      },
      "source": [
        "\n",
        "**Run this cell after each session restart**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkbcraCUaPEy"
      },
      "source": [
        "#@title General setup { display-mode: \"form\", run: \"auto\" }\n",
        "\n",
        "!pip install torch torchvision\n",
        "!pip install accelerate\n",
        "!pip install gputil\n",
        "\n",
        "# !pip install torchmetrics==0.10.3\n",
        "# !pip install pytorch-lightning==1.8.3.post0\n",
        "\n",
        "from IPython.display import HTML, Image, display, clear_output\n",
        "from moviepy.editor import ImageSequenceClip, ipython_display\n",
        "import ipywidgets as widgets\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "!apt-get -qq install ffmpeg\n",
        "!pip install ninja\n",
        "from google.colab import drive\n",
        "drive.mount('/G', force_remount=True)\n",
        "gdir = '/G/MyDrive/'\n",
        "%cd $gdir\n",
        "\n",
        "#@markdown Copying StyleGAN2 to the directory below on your Google drive (creating it, if it doesn't exist):\n",
        "work_dir = 'sdfu' #@param {type:\"string\"}\n",
        "#@markdown NB: Avoid connecting Google drive manually via the icon in Files section on the left. Doing so may break further operations.\n",
        "\n",
        "work_dir = gdir + work_dir + '/'\n",
        "if not os.path.isdir(work_dir):\n",
        "  !git clone https://github.com/eps696/SDfu $work_dir\n",
        "%cd $work_dir\n",
        "!pip install -r requirements.txt\n",
        "!pip install xformers\n",
        "# !pip install git+https://github.com/openai/CLIP.git@main#egg=clip\n",
        "\n",
        "from src.util.text import txt_clean\n",
        "from src.util.utils import basename\n",
        "from download import get_model\n",
        "maindir = '/content/models'\n",
        "command = ''\n",
        "args = ''\n",
        "\n",
        "clear_output()\n",
        "\n",
        "# Hardware check\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "import GPUtil as GPU\n",
        "gpu = GPU.getGPUs()[0]\n",
        "!nvidia-smi -L\n",
        "print(\"GPU RAM {0:.0f}MB | Free {1:.0f}MB)\".format(gpu.memoryTotal, gpu.memoryFree))\n",
        "print('\\nDone!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load models\n",
        "\n",
        "model = \"1.5-dreamlike\" #@param ['1.5','1.5-dreamlike','2.1','2.1-v','2-inpainting','1.5-instruct-pix2pix','Kandinsky 2.2']\n",
        "controlnet = False #@param {type:\"boolean\"}\n",
        "\n",
        "if '1.5' in model:\n",
        "  get_model(\"https://www.dropbox.com/s/9wuhum8w0iqs4o7/sdfu-v15-full-fp16.zip?dl=1\", '/content/models/v1')\n",
        "  get_model(\"https://www.dropbox.com/s/z9uycihl6tybx9y/sdfu-v1-vaes-fp16.zip?dl=1\", '/content/models/v1')\n",
        "if model == '1.5-dreamlike':\n",
        "  get_model(\"https://www.dropbox.com/s/7hwh27xfzcy7a2g/sdfu-v15drm-unetx-fp16.zip?dl=1\", '/content/models/v1')\n",
        "if model == '1.5-instruct-pix2pix':\n",
        "  get_model(\"https://www.dropbox.com/s/n1z21ds5eauzk4m/sdfu-ip2p-unet-fp16.zip?dl=1\", '/content/models/v1')\n",
        "if model in ['2.1', '2-inpainting']:\n",
        "  get_model(\"https://www.dropbox.com/s/38876tjuklvwq82/sdfu-v21-full-fp16.zip?dl=1\", '/content/models/v2')\n",
        "if model == '2-inpainting':\n",
        "  get_model(\"https://www.dropbox.com/s/r5fa1mdxpw9e8k2/sdfu-v2i-unet-fp16.zip?dl=1\", '/content/models/v2')\n",
        "if model == '2.1-v':\n",
        "  get_model(\"https://www.dropbox.com/s/10gbecrugca1ydv/sdfu-v21v-full-fp16.zip?dl=1\", '/content/models/v2v')\n",
        "\n",
        "if controlnet:\n",
        "  get_model(\"https://www.dropbox.com/s/qhe1zpbubjr3t75/sdfu-controlnet.zip?dl=1\", '/content/models')\n",
        "\n",
        "# clipseg for text masking\n",
        "get_model(\"https://www.dropbox.com/s/c0tduhr4g0al1cq/rd64-uni.pth?dl=1\", '/content/models/clipseg', unzip=False)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "A9a_9LXrV2dR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Njelbgu8APJ"
      },
      "source": [
        "## Generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Other settings [optional]"
      ],
      "metadata": {
        "id": "BCyeZMHNT2E-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "command = ''\n",
        "args = ''\n",
        "\n",
        "#@markdown Run this cell to override settings, if needed\n",
        "out_dir = '_out' #@param {type:\"string\"}\n",
        "sizeX = 768 #@param {type:\"integer\"}\n",
        "sizeY = 768 #@param {type:\"integer\"}\n",
        "\n",
        "cfg_scale = 7.5 #@param {type:\"number\"}\n",
        "strength = 0.7 #@param {type:\"number\"}\n",
        "steps = 50 #@param {type:\"integer\"}\n",
        "\n",
        "sampler = 'ddim' #@param ['ddim', 'pndm', 'euler', 'klms', 'euler_a']\n",
        "VAE = 'ema' #@param ['original', 'ema', 'mse']\n",
        "batch = 1 #@param {type:\"integer\"}\n",
        "seed = 696 #@param {type:\"integer\"}\n",
        "ddim_eta = 0. #@param {type:\"number\"}\n",
        "\n",
        "unprompt = 'low quality, poorly drawn, out of focus, blurry, tiled, segmented, oversaturated' #@param {type:\"string\"}\n",
        "verbose = True #@param {type:\"boolean\"}\n",
        "\n",
        "args = ''\n",
        "args += ' -o %s ' % out_dir\n",
        "args += ' -sz %d-%d ' % (sizeX, sizeY)\n",
        "args += ' -C %g ' % cfg_scale\n",
        "args += ' -f %g ' % strength\n",
        "args += ' -s %d ' % steps\n",
        "args += ' -sm %s ' % sampler\n",
        "args += ' --vae %s ' % VAE\n",
        "args += ' -b %d ' % batch\n",
        "args += ' -S %d ' % seed\n",
        "args += ' --ddim_eta %g ' % ddim_eta\n",
        "args += ' -un \"%s\" ' % unprompt\n",
        "if verbose:\n",
        "  args += ' -v '\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "B1B_woHjPvQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inputs"
      ],
      "metadata": {
        "id": "Ud1WQ0EqWLcw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwoOBOcR8APK",
        "cellView": "form"
      },
      "source": [
        "#@markdown All paths below are relative to the work directory on G drive (set during General setup above).\n",
        "\n",
        "#@markdown Specify a text string or path to a text file to use **txt2img**:\n",
        "prompt = '' #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Specify path to an image or directory to use **img2img**:\n",
        "images = '' #@param {type:\"string\"}\n",
        "#@markdown Specify mask as an image or directory or text string to use **inpainting**:\n",
        "mask = '' #@param {type:\"string\"}\n",
        "# mask_text = 'human, person' #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Load **finetuned** files if needed:\n",
        "method = \"custom\" #@param ['text','lora','custom']\n",
        "load_file = '' #@param {type:\"string\"}\n",
        "\n",
        "maindir = '/content/models'\n",
        "%cd $work_dir\n",
        "command = ' -md %s ' % maindir\n",
        "workname = ''\n",
        "\n",
        "if len(prompt) > 0:\n",
        "  if os.path.exists(prompt):\n",
        "    print('found', prompt)\n",
        "    command += ' -t %s ' % prompt\n",
        "  else:\n",
        "    command += ' -t \"%s\" ' % prompt\n",
        "  workname = txt_clean(basename(prompt))[:44]\n",
        "\n",
        "if len(images) > 0 and os.path.exists(images):\n",
        "  print('found', images)\n",
        "  command += ' -im %s ' % images\n",
        "  if len(workname) == 0:\n",
        "    workname = txt_clean(basename(images))\n",
        "else:\n",
        "  command += ' -f 1 '\n",
        "  print('Strength is set to 1 for txt2img method')\n",
        "\n",
        "if len(mask) > 0:\n",
        "  if not os.path.exists(mask):\n",
        "      mask = '\"%s\"' % mask\n",
        "  command += ' -M %s ' % mask\n",
        "\n",
        "if len(load_file) > 0 and os.path.exists(load_file):\n",
        "  print('found', load_file)\n",
        "  cmd = 'rt' if method=='text' else 'rl' if method=='lora' else 'rd'\n",
        "  command += ' -%s %s ' % (cmd, load_file)\n",
        "\n",
        "# !echo $command $args\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Add reference images to control output by **Controlnet** if needed:\n",
        "cnet_images = '' #@param {type:\"string\"}\n",
        "cnet_mode = \"depth\" #@param ['depth','canny','pose']\n",
        "cnet_strength = 0.7 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown Or do it with **Instruct pix2pix** (if scale > 0):\n",
        "ip2p_scale = 0. #@param {type:\"number\"}\n",
        "\n",
        "if len(cnet_images) > 0 and os.path.exists(cnet_images):\n",
        "  print('using Controlnet with', cnet_images)\n",
        "  %run src/preproc.py -i $cnet_images --type $cnet_mode -o /content/$cnet_mode\n",
        "  command += ' -cmod %s -cimg %s -cts %f ' % (cnet_mode, '/content/'+$cnet_mode, cnet_strength)\n",
        "\n",
        "elif ip2p_scale > 0:\n",
        "  if 'instruct-pix2pix' in model:\n",
        "    print('using Instruct-pix2pix with scale', ip2p_scale)\n",
        "    command += ' --img_scale %f ' % ip2p_scale\n",
        "  else:\n",
        "    print('Select Instruct-pix2pix model above!')\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nrQM2TOu6vnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate images"
      ],
      "metadata": {
        "id": "E60K0IQLdCMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Separate frames\n",
        "\n",
        "if \"Kandinsky\" in model:\n",
        "  %run src/kand.py $args $command\n",
        "else:\n",
        "  %run src/gen.py $args $command"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8TDpplliq8Yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Interpolations\n",
        "\n",
        "latent_blending = 0. #@param {type:\"number\"}\n",
        "frame_step = 10 #@param {type:\"integer\"}\n",
        "num_repeat = 3 #@param {type:\"integer\"}\n",
        "\n",
        "command += ' -fs %d -n %d ' % (frame_step, num_repeat)\n",
        "\n",
        "if latent_blending > 0:\n",
        "  command += ' --latblend %f' % latent_blending\n",
        "\n",
        "if \"Kandinsky\" in model:\n",
        "  %run src/kand.py $args $command\n",
        "else:\n",
        "  %run src/latwalk.py $args $command\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8KKZ7C_Iogpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "set `latent_blending` if you need smooth transitions.  \n",
        "`frame_step` = length of the transition between prompts or images (in frames).  \n",
        "`num_repeat` = multiplies inputs to make animation e.g. from a single prompt or image."
      ],
      "metadata": {
        "id": "PybkSeM0goXa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate video"
      ],
      "metadata": {
        "id": "VqKR1u-c-_YH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Load Zeroscope (low-res) and Potat1 (high-res) text2video models\n",
        "\n",
        "get_model(\"https://www.dropbox.com/s/38876tjuklvwq82/sdfu-v21-full-fp16.zip?dl=1\", '/content/models/v2')\n",
        "get_model(\"https://www.dropbox.com/s/uyaidznqjaot7hw/sdfu-video-unet-fp16.zip?dl=1\", '/content/models/v2')\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6FNkXo-wW_Od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Make short (~30 frames) videos.\n",
        "#@markdown If provided input video is longer, it will be cut in pieces and processed one by one.\n",
        "\n",
        "#@markdown Specify a text string:\n",
        "prompt = '' #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Specify path to the input video, if needed:\n",
        "video = '' #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Generate low-res or high-res version (or both):\n",
        "lo = True #@param {type:\"boolean\"}\n",
        "hi = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Other parameters:\n",
        "sampler = 'euler_a' #@param ['ddim', 'pndm', 'euler', 'klms', 'euler_a']\n",
        "frames = 36 #@param {type:\"integer\"}\n",
        "\n",
        "command = '-sm %s  -vf %s' % (sampler, frames)\n",
        "command += ' -md /content/models'\n",
        "# %cd $work_dir\n",
        "\n",
        "if len(prompt) > 0:\n",
        "  command += ' -t \"%s\" ' % prompt\n",
        "if len(video) > 0 and os.path.exists(video):\n",
        "  command += ' -iv %s ' % video\n",
        "if lo:\n",
        "  command += ' -m vzs '\n",
        "if hi:\n",
        "  command += ' -up vpot '\n",
        "\n",
        "if len(prompt) > 0 or (len(video) > 0 and os.path.exists(video)):\n",
        "  %run src/vid.py $command\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "bEtTjbGPyxS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWpWFeyO8APF"
      },
      "source": [
        "## Fine-tune"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEmNzQmm0t_o",
        "cellView": "form"
      },
      "source": [
        "#@title Data setup\n",
        "#@markdown Put your target images as zip-archive onto Google drive and type its path below (relative to G-drive root).\n",
        "#@markdown If you use Custom diffusion, prepare also a bunch of generic reference images of similar class, to start from.\n",
        "#@markdown `new_token` will be used in the prompts to summon your target imagery, e.g. as `<mycat1>`.\n",
        "new_token = 'mycat1' #@param {type:\"string\"}\n",
        "ref_class = 'cat' #@param {type:\"string\"}\n",
        "target_data = 'tgt.zip' #@param {type:\"string\"}\n",
        "ref_data = 'ref.zip' #@param {type:\"string\"}\n",
        "\n",
        "data_dir = os.path.join('/content/data/', new_token)\n",
        "!rm -rf $data_dir\n",
        "os.makedirs(data_dir)\n",
        "%cd $data_dir\n",
        "\n",
        "tgt_path = os.path.join(gdir, target_data)\n",
        "!unzip -j -o -q $tgt_path -d tgt\n",
        "tgt_dir = os.path.join(data_dir, 'tgt')\n",
        "\n",
        "ref_path = os.path.join(gdir, ref_data)\n",
        "if len(ref_data) > 0 and os.path.isfile(ref_path):\n",
        "  !unzip -j -o -q $ref_path -d ref\n",
        "  ref_dir = os.path.join(data_dir, 'ref')\n",
        "  with_prior = True\n",
        "else:\n",
        "  with_prior = False\n",
        "\n",
        "%cd $work_dir\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use either of methods:\n",
        "* [Textual inversion](https://textual-inversion.github.io) = adds new token to the text encoder. Generic but stable. Trained embeddings can be combined together on load.\n",
        "* [LoRA](https://github.com/cloneofsimo/lora) = adds new token + partially finetunes Unet attention layers. Faster, precise, but may interfere with wider spectrum of topics.\n",
        "* [Custom diffusion](https://github.com/adobe-research/custom-diffusion) = similar to LoRA (in a way). Can achieve impressive reproduction quality (including faces) with simple prompts, but may lose the point with too complex ones. To train it, you'll need to specify above both **target** reference images and **generic** ones (more random, of similar subjects). Apparently, you can generate the latter with SD itself.  \n",
        "\n",
        "Mark `style` if you're training for a style, rather than an object.  \n",
        "Mark `low_mem` if you get OOM.  "
      ],
      "metadata": {
        "id": "CsNRhrxr4aKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run training\n",
        "method = \"custom\" #@param ['text','lora','custom']\n",
        "style = False #@param  {type:\"boolean\"}\n",
        "low_mem = False #@param  {type:\"boolean\"}\n",
        "train_steps = 2000 #@param  {type:\"integer\"}\n",
        "save_steps = 500 #@param  {type:\"integer\"}\n",
        "batch = 1 #@param  {type:\"integer\"}\n",
        "\n",
        "command = ' -t $s --term %s --data %s ' % (new_token, ref_class, tgt_dir)\n",
        "command += ' -ts %d --save_step %d -b %d ' %(train_steps, save_steps, batch)\n",
        "command += ' -val -md %s ' % maindir\n",
        "\n",
        "if low_mem:\n",
        "  command += ' --low_mem '\n",
        "if style\n",
        "  command += ' --style '\n",
        "\n",
        "if method == 'text':\n",
        "  %run src/train.py $command -lr 0.001 --type text\n",
        "elif method == 'lora':\n",
        "  %run src/train.py $command -lr 0.0001 --type lora\n",
        "elif method == 'custom':\n",
        "  %run src/train.py $command --term_data $ref_dir --type custom\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "AW5--vadmcYb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}